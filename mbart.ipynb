{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12134671,"sourceType":"datasetVersion","datasetId":7641838}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\nfrom typing import Iterable, List\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\nfrom timeit import default_timer as timer\nfrom torch.nn import Transformer\nfrom torch import Tensor\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport math\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json\nfrom torch.optim import AdamW","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-11T20:08:22.043855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set seed.\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model name is mBART\nmodel_name = \"facebook/mbart-large-50-many-to-many-mmt\"\ntokenizer = MBart50TokenizerFast.from_pretrained(model_name)\ntraining_file_path = \"/kaggle/input/sam-data/sampleData.json\"\ncols = [\"ko_text\", \"en_text\"]\ntest_size = 0.25\n\nBATCH_SIZE = 1\nNUM_EPOCHS = 2\nif(torch.cuda.is_available()):\n    DEVICE = \"cuda\"\nelse:\n    DEVICE = \"cpu\"\nprint(f\"Using {DEVICE}\")\nmodel = MBartForConditionalGeneration.from_pretrained(model_name).to(DEVICE)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Total parameters and trainable parameters.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")\nprint(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataset load\nwith open(training_file_path, encoding = 'utf-8') as f:\n    json_data = json.load(f)\n# json_data\ntexts = [{col: item[col] for col in cols} for item in json_data[\"Text\"]]\n\ndf = pd.DataFrame(texts)\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.src_lang = \"ko_KR\"\ntokenizer.tgt_lang = \"en_XX\"\n#mBART use special language token for identification\nmodel.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(\"en_XX\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom Dataset class.\nclass TranslationDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length = 128):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        src = self.df[cols[0]].iloc[idx]\n        tgt = self.df[cols[1]].iloc[idx]\n\n        src_enc = self.tokenizer(src, return_tensors = \"pt\", padding=\"max_length\", truncation=True, max_length = self.max_length)\n        tgt_enc = self.tokenizer(tgt, return_tensors = \"pt\", padding=\"max_length\", truncation=True, max_length = self.max_length)\n\n        input_ids = src_enc[\"input_ids\"].squeeze()\n        attention_mask = src_enc[\"attention_mask\"].squeeze()\n        labels = tgt_enc[\"input_ids\"].squeeze()\n        labels[labels == self.tokenizer.pad_token_id] = -100 #ignore padding in loss calculation\n        return{\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\":labels\n        }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataset split into train and test data\ntrain_data, test_data = train_test_split(df, test_size = test_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = TranslationDataset(train_data, tokenizer)\nvalid_dataset = TranslationDataset(test_data, tokenizer)\niterator = iter(train_dataset)\nprint(next(iterator))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\ntrain_dataset[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model, optimizer, num_epochs, dataloader):\n    print('Training')\n    model.to(DEVICE)\n    model.train()\n\n    train_loss = []\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        num_iter = 0\n        for batch in dataloader:\n            input_ids = torch.tensor(batch['input_ids']).to(DEVICE)\n            attention_mask = torch.tensor(batch['attention_mask']).to(DEVICE)\n            labels = torch.tensor(batch['labels']).to(DEVICE)\n    \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n\n            epoch_loss += loss.item()\n            num_iter += 1\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            print(f\"Epoch {epoch}, iter = {num_iter}, Loss: {loss.item()}\")\n        train_loss.append(epoch_loss/num_iter)\n    return train_loss\n\ndef evaluate(model, dataloader):\n    print('Validating')\n    model.eval()\n    losses = 0\n    num_iter = 0\n    # valid_loss = []\n    for batch in dataloader:\n        input_ids = torch.tensor(batch['input_ids']).to(DEVICE)\n        attention_mask = torch.tensor(batch['attention_mask']).to(DEVICE)\n        labels = torch.tensor(batch['labels']).to(DEVICE)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        losses += loss.item()\n        num_iter += 1\n        print(f\"iter = {num_iter}, Loss: {loss.item()}\")\n    # valid_loss.append(epoch_loss/num_iter)\n    return losses/num_iter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nsrc_text = [\"안녕하세요. 만나서 반갑습니다.\"]\ninputs = tokenizer(src_text, return_tensors=\"pt\", padding=True)\n\ninputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(model, optimizer, NUM_EPOCHS, train_dataloader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate(model, valid_dataloader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#save the model\nmodel.save_pretrained(\"/kaggle/working/facebook/mbart-large-50-many-to-many-mmt-finetuning\")\n\n#save tokenizer\ntokenizer.save_pretrained(\"/kaggle/working/facebook/mbart-large-50-many-to-many-mmt-finetuning-token\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#load the model\nfinetuned_model = MBartForConditionalGeneration.from_pretrained(\"/kaggle/working/facebook/mbart-large-50-many-to-many-mmt-finetuning\").to(DEVICE)\nfinetuned_tokenizer = MBart50TokenizerFast.from_pretrained(\"/kaggle/working/facebook/mbart-large-50-many-to-many-mmt-finetuning-token\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"src_text = [\"안녕하세요. 만나서 반갑습니다.\"]\ninputs = tokenizer(src_text, return_tensors=\"pt\", padding=True).to(DEVICE)\n\noutputs = finetuned_model.generate(**inputs)\n\ntranslation = tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"translation","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}