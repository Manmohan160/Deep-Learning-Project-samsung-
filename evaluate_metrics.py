# evaluate_metrics.py

import pandas as pd
from sacrebleu import corpus_bleu
from bert_score import score

# âœ… Load translated file
df = pd.read_csv("conversation_titles_with_translation.csv")

# âœ… Extract references and predictions
references = df["eng_title"].fillna("").tolist()        # Ground truth
predictions = df["translated_en"].fillna("").tolist()   # mBART output

# âœ… BLEU Evaluation
bleu = corpus_bleu(predictions, [references])
print(f"ðŸ“˜ BLEU Score: {bleu.score:.2f}")

# âœ… BERTScore Evaluation
P, R, F1 = score(predictions, references, lang="en", verbose=True)
print(f"ðŸ”¬ BERTScore â†’ Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}")
