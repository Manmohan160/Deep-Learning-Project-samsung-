{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b3ba91",
   "metadata": {},
   "source": [
    "# ðŸ‡°ðŸ‡· Korean â†’ English Translation using T5 (Hugging Face Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328bac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Install libraries (only run once)\n",
    "!pip install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Load and prepare data from WikiMatrix\n",
    "import pandas as pd\n",
    "\n",
    "# Load your aligned Korean-English file (already unzipped)\n",
    "with open(\"./en-ko_unzipped/WikiMatrix.en-ko.en\", encoding=\"utf-8\") as f_en, \\\n",
    "     open(\"./en-ko_unzipped/WikiMatrix.en-ko.ko\", encoding=\"utf-8\") as f_ko:\n",
    "    en_lines = [line.strip() for line in f_en.readlines()]\n",
    "    ko_lines = [line.strip() for line in f_ko.readlines()]\n",
    "\n",
    "df = pd.DataFrame({\"en\": en_lines, \"ko\": ko_lines})\n",
    "df.dropna(inplace=True)\n",
    "df = df[(df['en'].str.strip() != \"\") & (df['ko'].str.strip() != \"\")]\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df = df[:5000]  # keep small for demo\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960dadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Load T5 tokenizer and model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = 'google/mt5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d655398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Tokenize the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.targets['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "# Format inputs for T5 (prefix task with \"translate Korean to English: ...\")\n",
    "df['src'] = 'translate Korean to English: ' + df['ko']\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['src'], df['en'], test_size=0.1, random_state=42)\n",
    "\n",
    "train_enc = tokenizer(list(X_train), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "train_labels = tokenizer(list(y_train), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "val_enc = tokenizer(list(X_val), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "val_labels = tokenizer(list(y_val), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = TranslationDataset(train_enc, train_labels)\n",
    "val_dataset = TranslationDataset(val_enc, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d785d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Train using Hugging Face Trainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c365952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Inference function\n",
    "def translate_ko_to_en(text):\n",
    "    input_text = \"translate Korean to English: \" + text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    output = model.generate(**inputs, max_length=128)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Try sample\n",
    "translate_ko_to_en(\"ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
